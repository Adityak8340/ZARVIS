# ZARVIS Architecture - LangGraph Agent System

## ğŸ¯ Overview

ZARVIS (Zero-Latency Autonomous Runtime Virtual Intelligence System) is a **LangGraph-based agent orchestrator** that intelligently coordinates multimodal AI capabilities through tools. The system uses a **Brain agent** as the central orchestrator with **Ear, Eye, and Mouth** as LangGraph tools.

---

## ğŸ—ï¸ Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ZARVIS                              â”‚
â”‚                    (Main Controller)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Brain Agent                               â”‚
â”‚              (LangGraph Orchestrator)                       â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Agent Node   â”‚â”€â”€â”€â”€â”€â–¶â”‚ Tool Router  â”‚â”€â”€â”€â”€â”€â–¶â”‚ Tools    â”‚  â”‚
â”‚  â”‚ (Reasoning)  â”‚â—€â”€â”€â”€â”€â”€â”‚ (Decision)   â”‚â—€â”€â”€â”€â”€â”€â”‚ (Execute)â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                                           â”‚       â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                             â”‚                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚                         â”‚
                 â–¼                         â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  LangGraph Toolsâ”‚      â”‚   Tool Node     â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚            â”‚            â”‚
    â–¼            â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ear   â”‚  â”‚   Eye   â”‚  â”‚  Mouth  â”‚
â”‚  Tool   â”‚  â”‚  Tool   â”‚  â”‚  Tool   â”‚
â”‚ (Listen)â”‚  â”‚  (See)  â”‚  â”‚ (Speak) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚            â”‚            â”‚
     â–¼            â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Groq    â”‚  â”‚ Groq    â”‚  â”‚ Groq    â”‚
â”‚ Whisper â”‚  â”‚ Vision  â”‚  â”‚ PlayAI  â”‚
â”‚  API    â”‚  â”‚  API    â”‚  â”‚  TTS    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
ZARVIS/
â”œâ”€â”€ main.py                      # Main ZARVIS controller with agent integration
â”œâ”€â”€ gui_main.py                  # GUI entry point
â”œâ”€â”€ test_agent.py                # Agent testing script
â”œâ”€â”€ requirements.txt             # Dependencies (includes LangGraph)
â”œâ”€â”€ README.md                    # Project overview
â”œâ”€â”€ ARCHITECTURE.md              # This file
â”œâ”€â”€ ARCHITECTURE_LANGGRAPH.md    # Detailed LangGraph implementation
â”œâ”€â”€ MIGRATION.md                 # Migration guide from old architecture
â”œâ”€â”€ SUMMARY.md                   # Project summary and statistics
â”œâ”€â”€ AUDIO_CLEANUP.md             # Audio cleanup feature documentation
â”œâ”€â”€ output/                      # Generated audio and recordings (auto-cleaned)
â””â”€â”€ src/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ brain_agent.py           # ğŸ§  LangGraph Agent Orchestrator (NEW)
    â”œâ”€â”€ brain_old.py             # ğŸ“¦ Original brain module (backup)
    â”œâ”€â”€ ear.py                   # ğŸ“¦ Original ear module (backup)
    â”œâ”€â”€ eye.py                   # ğŸ“¦ Original eye module (backup)
    â”œâ”€â”€ mouth.py                 # ğŸ“¦ Original mouth module (backup)
    â”œâ”€â”€ tools/                   # ğŸ”§ LangGraph Tools Package (NEW)
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ ear_tool.py          # ğŸ‘‚ Speech-to-Text Tool
    â”‚   â”œâ”€â”€ eye_tool.py          # ğŸ‘ï¸ Vision Analysis Tool
    â”‚   â””â”€â”€ mouth_tool.py        # ğŸ—£ï¸ Text-to-Speech Tool
    â””â”€â”€ gui/                     # ğŸ’» GUI Package
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ threads.py           # Background task threads
        â”œâ”€â”€ styles.py            # Dark theme and styling
        â”œâ”€â”€ chat_widget.py       # Chat interface component
        â”œâ”€â”€ settings_widget.py   # Settings tab component
        â”œâ”€â”€ audio_handler.py     # Audio recording/playback (with auto-cleanup)
        â””â”€â”€ main_window.py       # Main window coordinator
```

---

## ğŸ§  Brain Agent (Orchestrator)

**File:** `src/brain_agent.py`

### Purpose
The Brain is a **LangGraph-based intelligent orchestrator** that:
1. Receives user input
2. Decides which tools to use (Ear, Eye, Mouth)
3. Executes tools as needed
4. Generates final responses

### Key Components

#### 1. AgentState (TypedDict)
```python
class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    next: Optional[str]
```
- Manages conversation messages
- Tracks execution flow
- Uses LangChain message types

#### 2. StateGraph Architecture
```python
workflow = StateGraph(AgentState)
workflow.add_node("agent", self._agent_node)      # Reasoning
workflow.add_node("tools", ToolNode(self.tools))  # Tool execution
workflow.set_entry_point("agent")
workflow.add_conditional_edges("agent", self._should_continue, {...})
workflow.add_edge("tools", "agent")
```

#### 3. Execution Flow
```
User Input â†’ Agent Node â†’ Decision
                â†“
         [Tool Call Needed?]
           â†™         â†˜
         Yes          No
          â†“            â†“
      Tool Node    Response
          â†“
     Agent Node â†’ Final Response
```

### Methods

| Method | Purpose |
|--------|---------|
| `think(user_input, context)` | Process input through agent graph |
| `stream_think(user_input, context)` | Stream agent's reasoning process |
| `_agent_node(state)` | LLM reasoning with tool binding |
| `_should_continue(state)` | Routing logic for tool calls |

### System Prompt
```
You are ZARVIS, a Zero-Latency Autonomous Runtime Virtual Intelligence System.
You are a local, OS-integrated AI assistant that can see, hear, and speak.

You have access to these tools:
- listen_tool: Convert audio files to text (speech-to-text)
- see_tool: Analyze images and provide visual descriptions
- speak_tool: Convert text to speech audio files

Use these tools intelligently based on user requests.
Be concise, helpful, and action-oriented.
```

---

## ğŸ”§ Tools Package

### 1. Ear Tool (`src/tools/ear_tool.py`)

**Purpose:** Speech-to-Text conversion

```python
@tool
def listen_tool(audio_file_path: str, prompt: str = "Transcribe clearly") -> str:
    """
    Transcribe audio file to text using Groq Whisper.
    
    Use this tool when you need to convert speech/audio to text.
    """
    # Uses Groq Whisper Large V3
    # Returns transcribed text
```

**Capabilities:**
- Supports multiple audio formats (WAV, MP3, M4A, etc.)
- Context-aware transcription with prompt guidance
- Fast processing via Groq API
- Error handling and logging

**Usage:**
```python
# Agent automatically uses when it detects audio processing need
result = brain.think("Transcribe the audio at 'recording.m4a'")
```

---

### 2. Eye Tool (`src/tools/eye_tool.py`)

**Purpose:** Vision and Image Analysis

```python
@tool
def see_tool(image_url: str, prompt: str = "Describe what you see...") -> str:
    """
    Analyze an image and provide detailed observations using vision AI.
    
    Use this tool when you need to understand visual content.
    """
    # Uses Groq Vision (Llama 4 Scout)
    # Returns detailed image analysis
```

**Capabilities:**
- Analyzes images from URLs
- Identifies objects, text, scenes
- Provides detailed descriptions
- Understands visual context

**Usage:**
```python
# Agent automatically uses when it detects image analysis need
result = brain.think("What's in the image at 'https://example.com/photo.jpg'?")
```

---

### 3. Mouth Tool (`src/tools/mouth_tool.py`)

**Purpose:** Text-to-Speech synthesis

```python
@tool
def speak_tool(text: str, output_filename: str = "speech.wav", 
               voice: str = "Aaliyah-PlayAI") -> str:
    """
    Convert text to speech and save as an audio file.
    
    Use this tool when you need to generate speech audio.
    """
    # Uses Groq PlayAI TTS
    # Returns path to generated audio file
```

**Capabilities:**
- Generates natural speech from text
- Creates WAV audio files
- Customizable voice selection
- Saves to output directory

**Usage:**
```python
# Agent automatically uses when speech generation is needed
result = brain.think("Generate audio saying 'Hello World'")
```

---

## ğŸ’» GUI System

### Architecture
The GUI uses PyQt6 with a modular design pattern:

```
Main Window (Coordinator)
    â”œâ”€â”€ Chat Widget (User Interface)
    â”œâ”€â”€ Settings Widget (Configuration)
    â”œâ”€â”€ Audio Handler Mixin (Voice I/O)
    â”œâ”€â”€ Processing Threads (Background Tasks)
    â””â”€â”€ Media Player (Audio Playback)
```

### Key Components

#### 1. Main Window (`src/gui/main_window.py`)
- Application coordinator
- Window setup and layout
- Tab management
- Event handling
- Thread lifecycle management

#### 2. Chat Widget (`src/gui/chat_widget.py`)
- Text input/output
- Microphone button
- Image attachment button
- Voice toggle
- Message display with color coding

#### 3. Audio Handler (`src/gui/audio_handler.py`)
- Recording management
- Audio playback
- Speech generation
- **Automatic file cleanup** ğŸ§¹

#### 4. Processing Threads (`src/gui/threads.py`)
- `ProcessingThread` - AI tasks (text, voice, vision, speech)
- `AudioRecorder` - Microphone recording
- Non-blocking UI operations

#### 5. Styles (`src/gui/styles.py`)
- Dark theme stylesheet
- Color constants
- Font configurations

---

## ğŸ§¹ Audio Cleanup Feature

**Implemented in:** `src/gui/audio_handler.py`

### Purpose
Automatically delete temporary audio files during runtime to prevent disk space buildup.

### What Gets Cleaned Up

1. **Generated Speech Files** (from text-to-speech)
   - Deleted 200ms after playback completes
   - Example: `speech_123456.wav`

2. **Recorded Voice Files** (from microphone)
   - Deleted 500ms after transcription completes
   - Example: `recording_1699567890000.wav`

### How It Works

#### For Generated Speech:
```
User Message â†’ Agent Response â†’ speak_tool creates audio 
â†’ Audio plays â†’ Playback ends â†’ 200ms delay â†’ File deleted âœ“
```

#### For Voice Recordings:
```
User records â†’ File saved â†’ Transcribed by agent â†’ Response shown 
â†’ 500ms delay â†’ Recording deleted âœ“
```

### Implementation
```python
def _cleanup_audio_file(self):
    """Delete the current audio file after playback."""
    if hasattr(self, 'current_audio_file') and self.current_audio_file:
        audio_path = Path(self.current_audio_file)
        if audio_path.exists():
            time.sleep(0.1)  # Ensure file is released
            audio_path.unlink()
            print(f"âœ“ Cleaned up audio file: {audio_path.name}")
```

---

## ğŸ”„ Interaction Flows

### Text Command
```
1. User types message
2. ZARVIS.process_text_command()
3. Brain Agent processes
4. Agent Node reasons
5. Direct response (no tools)
6. Response displayed
```

### Voice Command
```
1. User clicks microphone
2. AudioRecorder records
3. Audio saved to output/
4. ZARVIS.process_voice_command()
5. Brain Agent receives prompt
6. Agent uses listen_tool
7. Tool Node transcribes
8. Agent processes text
9. Response generated
10. Recording deleted âœ“
```

### Image Analysis
```
1. User attaches image
2. ZARVIS.analyze_image()
3. Brain Agent receives prompt with URL
4. Agent uses see_tool
5. Tool Node analyzes image
6. Agent synthesizes insights
7. Analysis returned
```

### Voice Response
```
1. Agent generates text response
2. Voice enabled check
3. GUI triggers speak_tool
4. Audio file created
5. Media player plays
6. Playback completes
7. Audio file deleted âœ“
```

---

## ğŸ¯ Key Benefits of LangGraph Architecture

### 1. **Intelligent Orchestration**
- Agent automatically decides which tools to use
- No manual routing logic needed
- Dynamic tool selection based on context

### 2. **Scalability**
- Easy to add new tools (just use `@tool` decorator)
- Tools are self-contained modules
- Agent learns new capabilities automatically

### 3. **Maintainability**
- Clear separation of concerns
- Tools are independently testable
- Agent logic is centralized

### 4. **Flexibility**
- Support for complex workflows
- Streaming responses
- Conditional tool execution

### 5. **Observability**
- Full message history in state
- Observable tool calls
- Debug-friendly architecture

### Console Output Example:
```
ğŸ§  [Brain] Processing: Analyze this image...
ğŸ”§ [Brain] Routing to tools: ['see_tool']
ğŸ‘ï¸ [Eye Tool] Analyzed image, returned 250 characters
âœ“ [Brain] Execution complete
âœ“ Cleaned up audio file: speech_123456.wav
```

---

## ğŸ“¦ Dependencies

### Core
```txt
groq==0.33.0                    # Groq API for LLM, Vision, TTS, STT
python-dotenv==1.2.1            # Environment variables
```

### LangGraph & LangChain
```txt
langgraph==1.0.2                # Agent orchestration framework
langchain-core==0.3.22          # Core abstractions
langchain-groq==1.0.0           # Groq integration
langchain==1.0.5                # Full library
```

### GUI
```txt
PyQt6==6.10.0                   # Desktop interface
pillow==12.0.0                  # Image processing
sounddevice==0.5.3              # Audio recording
soundfile==0.13.1               # Audio file handling
```

---

## ğŸš€ Usage Examples

### 1. Basic Text Interaction
```python
from main import ZARVIS

zarvis = ZARVIS()
response = zarvis.process_text_command("Hello ZARVIS!")
print(response)
```

### 2. Voice Command Processing
```python
zarvis = ZARVIS()
response = zarvis.process_voice_command(
    "recording.m4a",
    generate_speech=True
)
print(response)
```

### 3. Image Analysis
```python
zarvis = ZARVIS()
result = zarvis.analyze_image(
    "https://example.com/image.jpg",
    "What objects do you see?"
)
print(result)
```

### 4. Multimodal Interaction
```python
zarvis = ZARVIS()
response = zarvis.multimodal_interaction(
    text="Analyze both inputs",
    audio="voice.m4a",
    image="https://example.com/photo.jpg"
)
print(response)
```

### 5. Direct Agent Access
```python
from src.brain_agent import Brain

brain = Brain()

# Natural language - agent decides everything!
response = brain.think(
    "Listen to audio.m4a, analyze image.jpg, "
    "and create a speech response"
)
print(response)
```

---

## ğŸ”§ Adding New Tools

### Step 1: Create Tool
```python
# src/tools/my_tool.py
from langchain_core.tools import tool

@tool
def my_new_tool(param: str) -> str:
    """
    Description of what this tool does.
    The agent will read this to understand when to use it.
    """
    # Implementation
    return result
```

### Step 2: Register in Brain
```python
# src/brain_agent.py
from src.tools.my_tool import my_new_tool

# In __init__:
self.tools = [listen_tool, see_tool, speak_tool, my_new_tool]
```

### Step 3: Done!
The agent will automatically learn to use your new tool based on its docstring!

---

## ğŸ“ Technical Details

### LangGraph Flow
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User Input        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Agent Node        â”‚
â”‚   (Reasoning)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Tool Call?  â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚
   Yes           No
    â”‚             â”‚
    â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Tools  â”‚   â”‚  END    â”‚
â”‚  Node   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Agent Node  â”‚
â”‚ (Synthesis) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  END    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Message Types
- `HumanMessage` - User input
- `AIMessage` - Agent responses
- `SystemMessage` - System prompts
- `ToolMessage` - Tool execution results

### State Management
```python
class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    next: Optional[str]
```

---

## ğŸ“Š Performance Characteristics

### Latency
- **LLM Response:** ~1-2 seconds (Groq optimized)
- **Speech-to-Text:** ~0.5-1 second
- **Text-to-Speech:** ~1-2 seconds
- **Vision Analysis:** ~1-2 seconds

### Resource Usage
- **Memory:** ~200-500 MB (depending on GUI)
- **CPU:** Minimal (API-based processing)
- **Disk:** Auto-cleanup keeps it minimal
- **Network:** Requires internet for Groq API

---

## ğŸ”’ Security & Privacy

### API Key Management
- Stored in `.env` file (not committed to git)
- Loaded via `python-dotenv`
- Never exposed in logs

### File Cleanup
- Automatic deletion of temporary audio files
- No persistent storage of voice recordings
- Output directory kept clean

### Tool Safety
- Tools have clear permissions and descriptions
- Agent can only use registered tools
- No arbitrary code execution

---

## ğŸ› Troubleshooting

### Common Issues

1. **Import Errors**
```bash
pip install --upgrade langgraph langchain-core langchain-groq
```

2. **API Key Missing**
```bash
# Check .env file
GROQ_API_KEY=your_key_here
```

3. **Audio Playback Issues**
- Check file permissions in `output/` directory
- Ensure media player has access to files
- Check console for cleanup logs

4. **Tool Not Being Called**
- Check tool is registered in `brain_agent.py`
- Verify tool docstring is clear
- Check console for routing decisions

---

## ğŸ“š Documentation Files

| File | Purpose |
|------|---------|
| `ARCHITECTURE.md` | This file - Complete system architecture |
| `ARCHITECTURE_LANGGRAPH.md` | Detailed LangGraph implementation guide |
| `MIGRATION.md` | Migration guide from old architecture |
| `SUMMARY.md` | Project statistics and overview |
| `AUDIO_CLEANUP.md` | Audio cleanup feature documentation |
| `README.md` | Quick start and project overview |

---

## ğŸ”® Future Enhancements

### Planned Features

1. **Persistent Memory**
   - Conversation history across sessions
   - User preferences and context
   - Checkpointing for long conversations

2. **Advanced Tool Composition**
   - Sequential tool chains
   - Parallel tool execution
   - Tool result aggregation

3. **Custom Nodes**
   - Decision-making nodes
   - Validation nodes
   - Post-processing nodes

4. **Enhanced Error Handling**
   - Retry logic for failed tools
   - Fallback strategies
   - Graceful degradation

5. **Performance Optimization**
   - Caching tool results
   - Lazy tool loading
   - Batch processing

---

## ğŸ“ Version History

### v2.0.0 - LangGraph Refactoring (Current)
- âœ… Migrated to LangGraph architecture
- âœ… Brain as agent orchestrator
- âœ… Ear, Eye, Mouth as tools
- âœ… Intelligent tool selection
- âœ… Automatic audio cleanup
- âœ… Comprehensive documentation

### v1.0.0 - Original Architecture
- Basic module structure
- Direct function calls
- Manual orchestration
- Manual file management

---

## ğŸ¤ Contributing

When adding new features:
1. Create tools with `@tool` decorator
2. Add clear docstrings (agent reads these!)
3. Test tools independently
4. Update documentation
5. Submit PR with examples

---

## ğŸ“ Support

For questions or issues:
1. Check this documentation
2. Review `ARCHITECTURE_LANGGRAPH.md` for implementation details
3. Check `MIGRATION.md` for usage examples
4. Look at console logs for debugging

---

**ZARVIS v2.0 - Zero-Latency Autonomous Runtime Virtual Intelligence System**

**Built with â¤ï¸ using LangGraph, LangChain, and Groq**
