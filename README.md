# ğŸ§  ZARVIS â€” Zero-Latency Autonomous Runtime Virtual Intelligence System

ZARVIS is a **local, OS-integrated AI assistant** that can **see, hear, speak, and act**.
It's not a chatbot â€” it's a **runtime intelligence layer** that perceives your environment, understands context, and executes tasks directly on your system.

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![PyQt6](https://img.shields.io/badge/GUI-PyQt6-green)](https://www.riverbankcomputing.com/software/pyqt/)
[![Groq](https://img.shields.io/badge/Powered%20by-Groq-orange)](https://groq.com/)

---

## ğŸš€ Quick Start

### Installation

**Windows (PowerShell):**
```powershell
.\setup.ps1
```

**Linux/Mac:**
```bash
chmod +x setup.sh
./setup.sh
```

### Configuration

1. Get your Groq API key from [console.groq.com](https://console.groq.com/)
2. Add it to `.env`:
```
GROQ_API_KEY=your_api_key_here
```

### Run

**With GUI (Recommended):**
```bash
python gui_main.py
```

**Command Line:**
```bash
python main.py
```

---

## âš™ï¸ Core Features

### ğŸ—£ï¸ Voice

* Natural voice interaction (speech-to-text + text-to-speech)
* Always-on or push-to-talk listening
* Personalized voice responses

### ğŸ‘‚ Ear

* Real-time speech recognition
* Wake-word detection (â€œHey Zarvisâ€)
* Context-aware command parsing

### ğŸ‘ï¸ Vision

* Screen awareness through OCR and image analysis
* Detects text, windows, and visual elements
* Understands on-screen context for decision-making

### ğŸ§  Brain

* LLM-powered reasoning and decision logic
* Short- and long-term memory (context retention)
* Adaptive responses and behavior based on history

### âš¡ Action Engine

* Executes OS commands and automations
* Controls files, apps, and processes
* Integrates with plugins for extended capabilities

### ğŸ”Œ Plugins

* Modular skill system with manifest-based permissions
* Loadable extensions for specific domains (e.g., Git, browser, monitoring)

---

## ğŸ§© Architecture

* **Frontend:** PyQt6 desktop interface with dark theme
* **Core:** Python runtime for logic, AI, and memory
* **Input Systems:** Voice + Vision modules
* **Output Systems:** Speech + GUI responses
* **Action Layer:** Secure OS control and automation
* **API:** Groq for LLM, Vision, TTS, and STT

### Module Structure

```
ZARVIS/
â”œâ”€â”€ main.py              # CLI entry point
â”œâ”€â”€ gui_main.py          # GUI entry point
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ brain.py         # LLM reasoning (Llama 4 Scout)
â”‚   â”œâ”€â”€ eye.py           # Vision analysis (Llama Vision)
â”‚   â”œâ”€â”€ ear.py           # Speech-to-text (Whisper)
â”‚   â”œâ”€â”€ mouth.py         # Text-to-speech (PlayAI)
â”‚   â””â”€â”€ gui/             # ğŸ“¦ Modular PyQt6 interface
â”‚       â”œâ”€â”€ __init__.py         # Package entry point
â”‚       â”œâ”€â”€ threads.py          # Background processing
â”‚       â”œâ”€â”€ styles.py           # UI theming
â”‚       â”œâ”€â”€ chat_widget.py      # Chat interface
â”‚       â”œâ”€â”€ settings_widget.py  # Settings tab
â”‚       â”œâ”€â”€ audio_handler.py    # Voice recording/playback
â”‚       â””â”€â”€ main_window.py      # Main coordinator
â”œâ”€â”€ output/              # Generated audio files
â”œâ”€â”€ ARCHITECTURE.md      # ğŸ“– Detailed code structure docs
â””â”€â”€ .env                 # API configuration
```

**âœ¨ New: Modular Architecture!**
- Each GUI file is ~43-147 lines (easy to read and maintain)
- Clear separation of concerns
- Easy to extend with new features
- See [ARCHITECTURE.md](ARCHITECTURE.md) for details

---

## ğŸ–¥ï¸ GUI Features

- **ğŸ’¬ Unified Chat Interface**: 
  - Text, voice, and vision all in one place
  - ğŸ¤ Microphone button for live voice recording
  - ï¿½ï¸ Image button for vision analysis
  - ğŸ—£ï¸ Toggle for automatic voice responses
- **âš™ï¸ Settings Tab**: Model info and memory management
- **Dark Theme**: Professional, eye-friendly interface
- **Multi-threaded**: Non-blocking UI during AI processing
- **Modular Code**: Clean architecture with focused components

---

## ğŸ“‹ Requirements

- Python 3.8+
- Groq API key (free at [console.groq.com](https://console.groq.com/))
- Windows/Linux/Mac
- 2GB RAM minimum

---

## ğŸ§  Philosophy

ZARVIS isn't built to chat â€” it's built to **think, perceive, and act locally**.
It merges **voice, vision, and reasoning** to create an intelligent, autonomous assistant that operates directly within your system environment.

> "Not just an assistant â€” your OS's second brain."

---

## ğŸ“š Documentation

- [GUI Guide](README_GUI.md) - Detailed GUI usage instructions
- [API Documentation](https://console.groq.com/docs) - Groq API reference

---

## ğŸ› ï¸ Development

### Adding New Features

Each cognitive module is independent and can be extended:

```python
from src.brain import Brain
from src.eye import Eye

brain = Brain()
eye = Eye()

# Multimodal interaction
vision_data = eye.see("image_url.jpg")
response = brain.think("Analyze this", context={"vision": vision_data})
```

### Running Tests

```bash
python -m pytest tests/
```

---

## ğŸ¤ Contributing

Contributions welcome! Please read our contributing guidelines and submit PRs.

---

## ğŸ“ License

MIT License - See LICENSE file for details

---

## ğŸ™ Acknowledgments

- Powered by [Groq](https://groq.com/) for ultra-fast AI inference
- Built with [PyQt6](https://www.riverbankcomputing.com/software/pyqt/) for the GUI
- Models: Llama 4 Scout, Whisper v3, PlayAI TTS
